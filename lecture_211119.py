# -*- coding: utf-8 -*-
"""Lecture_211119.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18expXjW59V4x2Qhs2f9stlGyPqeCSpHg
"""

import tensorflow as tf
import numpy as np

# mnist data 학습
# 16 epoch, 32 batch_size
# 5개의 dense 층만 사용
# 모든 dense층에 tf.random_uniform_initializer(minval = ?, maxval = ?, seed = None) 사용
# activation은 relu 또는 sigmoid 사용
# dropout, data argumentation 사용 가능

# 제출은 .py 코드와 PDF 캡처

# 다층구조 학습 x
# 1. SGD + GPU
# 2 .ReLu activation function
# 3.hrlghk Xavier/glorot 초기화, (제일 중요)
# .. datanormalization

# Xavier 기분 : input variance == output variance
# variance = ?
t   = [3, 4, 5, 6, 7] # larger variance
t2  = [5, 5, 5, 5, 5] # smaller variance
m   = (3+4+5+6+7) # 평균 값
var_t = ( (3-m)*(3-m) + (4-m)*(3-m) + (5-m)*(5-m) + (6-m)*(6-m) + (7-m)*(7-m) ) / 5
# variance는 원소와 평균 차이 제곱의 평균, 원소 값의 차이를 대표하는 수
print(var_t)
print(np.var(t))

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train/255.0, x_test/255.0 # No data normalization

print(np.var(x_train[0:32]))

uniform_init1 = tf.random_uniform_initializer(minval = -0.35, maxval = 0.35, seed = None)
uniform_init2 = tf.random_uniform_initializer(minval = -0.50, maxval = 0.50, seed = None)
uniform_init3 = tf.random_uniform_initializer(minval = -0.10, maxval = 0.10, seed = None)
mnistM = tf.keras.models.Sequential([
                                     tf.keras.layers.Flatten(input_shape=(28,28)),
                                     tf.keras.layers.Dense(128, activation = 'sigmoid', kernel_initializer= uniform_init1),
                                     tf.keras.layers.Dense(128, activation = 'sigmoid', kernel_initializer= uniform_init2),
                                     tf.keras.layers.Dense(10, activation = None, kernel_initializer= uniform_init3)
                                     #tf.keras.layers.Dense(128, activation = 'sigmoid', kernel_initializer=uniform_init),
                                     #tf.keras.layers.Dense(10, activation = None, kernel_initializer = uniform_init ) # (784×10) + 10
])
r = mnistM(x_train[0:32])
print("input variance: ", np.var(x_train[0:32]))
print("output variance: ", np.var(r))

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)
mnistM.compile(optimizer = 'SGD', loss = loss_fn, metrics = ['accuracy'])

mnistM.fit(x_train, y_train, epochs = 8, batch_size = 32)

