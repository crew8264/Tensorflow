# -*- coding: utf-8 -*-
"""Lecture_211126(R).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12V2yFtZTHgSDSujE3voGbfvLMiH9tPF0
"""

## Report ##

# batch size = 16, epoch=32
# train set을 2048개만 사용, validation set은 1024개만 사용하여 학습
# validation set에서 정확도(Accuracy)가 제일 높을 때 모델 저장
# model.save_wehights(..,), model.load_weights(...)
# 저장된 모델을 load하여 test 정확도 확인

"""TensorFlow를 프로그램으로 가져옵니다."""

import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow.keras.layers import Dense, Flatten, Conv2D
#tf.keras.layer.Dense <=> Dense
from tensorflow.keras import Model
#tf.keras.Model <=> Model

"""[MNIST 데이터셋](http://yann.lecun.com/exdb/mnist/)을 로드하여 준비합니다."""

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 # normalization

# Add a channels dimension #
print('NHW식 data: N of samples, H 이미지 height, W 이미지의 width', x_train.shape, x_test.shape)
x_train = x_train[..., tf.newaxis].astype("float32")
x_test = x_test[..., tf.newaxis].astype("float32")
print('NHWC 모양, C channel: ', x_train.shape, x_test.shape)
# NCHW 모양 사용하기도 함.

"""tf.data를 사용하여 데이터셋을 섞고 배치를 만듭니다:"""

# 학습하기 편한 객체로 변환, batch size를 16개로 지정
train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(10000).batch(16)

test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(16)

## added
# train_ds  = tfds.load('mnist', split = 'train[:2048]')

"""케라스(Keras)의 [모델 서브클래싱(subclassing) API](https://www.tensorflow.org/guide/keras#model_subclassing)를 사용하여 `tf.keras` 모델을 만듭니다:"""

class MyModel(Model): # MyModel은 tensorflow.keras.Model을 상속 받음.
  def __init__(self): # 생성자 함수, self <=> C++의 *this
    super(MyModel, self).__init__() # 상속 받은 class 생성자 함수 실행
    # MyModel class의 멤버 변수 선언 및 초기화
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10)

  def call(self, x): # forwarrd pass 정의, x는 input feature
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)

# Create an instance of the model
model = MyModel() # MyModel은 class, model은 MyModel 객체(변수): C++: MyModel model;
# prediction = model(x_train[0:1])

"""훈련에 필요한 옵티마이저(optimizer)와 손실 함수를 선택합니다: """

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam() # Adam()은 SGD()기반 최적화 함수

"""모델의 손실과 성능을 측정할 지표를 선택합니다. 에포크가 진행되는 동안 수집된 측정 지표를 바탕으로 최종 결과를 출력합니다."""

# metric  : loss  값, accuracy 정확도
train_loss = tf.keras.metrics.Mean(name='train_loss') # train_loss는 평균(Mean)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss') # test_loss는 평균(Mean)
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

"""`tf.GradientTape`를 사용하여 모델을 훈련합니다:"""

@tf.function # class 상속과 비슷하게 function(함수) 상속 (functional programing VS 객체지향 프로그래밍)
def train_step(images, labels): # batch 크기의 images=x, labels= y 로 학습
  with tf.GradientTape() as tape: # 아래 forward를 하면서  tape 계산, tape는 gradient계산에 필요한 값들.
    predictions = model(images, training=True) # forward pass
    loss = loss_object(labels, predictions) # loss 값/객체
  gradients = tape.gradient(loss, model.trainable_variables) # backward pass ( back propagation)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # 학습가능한 변수 업데이트

  train_loss(loss) # loss 값을 평균 train_loss에 합류하여 평균값 업데이트
  train_accuracy(labels, predictions) # batch 크기의 정확도를 가지고 전체 정확도 업데이트

"""이제 모델을 테스트합니다:"""

@tf.function # class 상속과 비슷하게 function(함수) 상속 (functional programing VS 객체지향 프로그래밍)
def test_step(images, labels): # batch 크기의 images=x, labels=y 로 학습
  # training=False is only needed if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  predictions = model(images, training=False) # forward pass 
  t_loss = loss_object(labels, predictions) # loss 값/객체 계산

  test_loss(t_loss) # batch 크기의 t_loss값을 평균 test_loss에 합류하여 평균값 업데이트
  test_accuracy(labels, predictions) # batch 크기의 정확도를 가지고 전체 test data 정확도 업데이트

EPOCHS = 32

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states() # train_loss 초기화
  train_accuracy.reset_states() # train_accuracy 초기화 # of samples=0, # of correct = 0
  test_loss.reset_states()
  test_accuracy.reset_states()

  
  for images, labels in train_ds:  # batch크기 만큼 train_ds에서 (images, labels) 가져옴
    train_step(images, labels)

  for test_images, test_labels in test_ds: # batch크기 만큼 test_ds에서 (images, labels) 가져옴
    test_step(test_images, test_labels)

  print(
    f'Epoch {epoch + 1}, '
    f'Train Loss: {train_loss.result()}, '
    f'Train Accuracy: {train_accuracy.result() * 100}, '
    f'Test Loss: {test_loss.result()}, '
    f'Test Accuracy: {test_accuracy.result() * 100}'
  )